{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-shooting",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "In this notebook, we apply common preprocessing techniques to improve the quality of the collected data and extract the relevant features to be exploited by the trained ML algorithms. All of such transformations are implemented as custom `Transformer`s and `Estimator`s that are then applied as a Spark `Pipeline` to both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pharmaceutical-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel, Transformer, Estimator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-category",
   "metadata": {},
   "source": [
    "## Input data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesser-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationsData = spark.read.load(\"/user/garza/LabReply/ProjectData/stations.csv\", format=\"csv\", delimiter=\"\\t\", header=True, inferSchema=True)\n",
    "stationsData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concerned-julian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StationId: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- NumBikes-4: integer (nullable = true)\n",
      " |-- NumBikes-3: integer (nullable = true)\n",
      " |-- NumBikes-2: integer (nullable = true)\n",
      " |-- NumBikes-1: integer (nullable = true)\n",
      " |-- NumBikes: integer (nullable = true)\n",
      " |-- NumBikes+1: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData = spark.read.load(\"/user/garza/LabReply/ProjectData/trainingData.csv\", format=\"csv\", header=True, inferSchema=True)\n",
    "joinedTrainData = trainData.join(stationsData, trainData.StationId == stationsData.id)\n",
    "joinedTrainData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "concrete-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StationId: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- NumBikes-4: integer (nullable = true)\n",
      " |-- NumBikes-3: integer (nullable = true)\n",
      " |-- NumBikes-2: integer (nullable = true)\n",
      " |-- NumBikes-1: integer (nullable = true)\n",
      " |-- NumBikes: integer (nullable = true)\n",
      " |-- NumBikes+1: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData = spark.read.load(\"/user/garza/LabReply/ProjectData/testData.csv\", format=\"csv\", header=True, inferSchema=True)\n",
    "joinedTestData = testData.join(stationsData, testData.StationId == stationsData.id)\n",
    "joinedTestData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-authentication",
   "metadata": {},
   "source": [
    "## Diff features extraction\n",
    "\n",
    "As explained in the data exploration notebook, in order to have comparable results with stations with different sizes, we are going to conduct the analysis on the \"diff\" features, obtained as the first-order difference between pairs of consecutive readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "offensive-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffFeaturesExtractor(Transformer):\n",
    "    def _transform(self, inputDF):\n",
    "        return inputDF \\\n",
    "            .withColumn(\"NumBikesDiff-3\", (F.col(\"NumBikes-3\") - F.col(\"NumBikes-4\"))) \\\n",
    "            .withColumn(\"NumBikesDiff-2\", (F.col(\"NumBikes-2\") - F.col(\"NumBikes-3\"))) \\\n",
    "            .withColumn(\"NumBikesDiff-1\", (F.col(\"NumBikes-1\") - F.col(\"NumBikes-2\"))) \\\n",
    "            .withColumn(\"NumBikesDiff\", (F.col(\"NumBikes\") - F.col(\"NumBikes-1\"))) \\\n",
    "            .withColumn(\"NumBikesDiff+1\", (F.col(\"NumBikes+1\") - F.col(\"NumBikes\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-shuttle",
   "metadata": {},
   "source": [
    "## Time features extraction\n",
    "\n",
    "As shown in the data exploration notebook, the data appear to present some sort of trend when considering different pieces of the associated timestamp information. Those include the timestamp hour, day of the week, and the type of day (work day or non-work day). Thus, it makes sense to extract those pieces of information from our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brown-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeaturesExtractor(Transformer):\n",
    "    def _transform(self, inputDF):\n",
    "        return inputDF \\\n",
    "            .withColumn(\"Hour\", F.hour(\"Timestamp\")) \\\n",
    "            .withColumn(\"DayOfWeek\", ((F.dayofweek(\"Timestamp\") + 5) % 7).alias(\"DayOfWeek\")) \\\n",
    "            .withColumn(\"WorkDay\", F.when(F.col(\"DayOfWeek\") <= 5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-expense",
   "metadata": {},
   "source": [
    "## Location features extraction\n",
    "\n",
    "To leverage on the information related to the location of stations, a K-means clustering procedure has been applied, with K = 50, to the stations dataset; each station is then associated to a \"zone\" identifier, which may be used as a categorical feature in our regression problem.\n",
    "\n",
    "The assumption is that stations that are close to each other are more likely to be associated with a similar behavior with respect to bike availability, as reflected also in the visualization presented in the data exploration notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entertaining-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationFeaturesExtractorModel(Transformer):\n",
    "    def __init__(self, clusteredStationsDF):\n",
    "        self.clusteredStationsDF = clusteredStationsDF\n",
    "    \n",
    "    def _transform(self, inputDF):\n",
    "        return inputDF.join(self.clusteredStationsDF, \"StationId\")\n",
    "\n",
    "class LocationFeaturesExtractor(Estimator):\n",
    "    def _fit(self, inputDF):\n",
    "        clusterPipeline = Pipeline(stages=[\n",
    "            VectorAssembler(inputCols=[\"longitude\", \"latitude\"], outputCol=\"features\"),\n",
    "            KMeans(k=50, predictionCol=\"Zone\", seed=42)\n",
    "        ])\n",
    "        \n",
    "        return LocationFeaturesExtractorModel(\n",
    "            clusterPipeline.fit(inputDF.filter(F.col(\"id\") <= 283)).transform(inputDF.filter(F.col(\"id\") <= 283)).select(F.col(\"id\").alias(\"StationId\"), \"Zone\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-mainstream",
   "metadata": {},
   "source": [
    "## Training set summarizing\n",
    "\n",
    "The features described above (the hour, day of the week, and location) cannot be used directly in a regression problem. Indeed, they are categorical features: this means that their numeric encoding does not really carry any meaning that is related to the variable we are performing regression on, so it could lead to wrong results to feed them directly to the regression algorithm.\n",
    "\n",
    "There are multiple ways to address this issue. For example, a commonly used procedure is known as *one-hot encoding*, which encodes categorical features as a series of binary (0-1) dummy features, which allow to analyze independently the effect of having a specific value for the feature under analysis with respect to a baseline, at the cost of significantly increasing the dimension and the sparsity of the feature space.\n",
    "\n",
    "Another option, which is used here, is to summarize the values of the dependent variable in our entire training set separately by each categorical variable, producing statistics that can then be used as features for our test set (i.e., *target encoding*). To that we can also add \"joint\" summarizations, which take into account all the categorical variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "radio-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSetSummarizerModel(Transformer):\n",
    "    def __init__(self, avgByHour, avgByWeekday, avgByZone, avgCombined):\n",
    "        self.avgByHour = avgByHour\n",
    "        self.avgByWeekday = avgByWeekday\n",
    "        self.avgByZone = avgByZone\n",
    "        self.avgCombined = avgCombined\n",
    "        \n",
    "    def _transform(self, inputDF):\n",
    "        return inputDF \\\n",
    "            .join(self.avgByHour, \"Hour\") \\\n",
    "            .join(self.avgByWeekday, \"DayOfWeek\") \\\n",
    "            .join(self.avgByZone, \"Zone\") \\\n",
    "            .join(self.avgCombined, [\"Hour\", \"DayOfWeek\", \"Zone\"])\n",
    "\n",
    "class TrainingSetSummarizer(Estimator):\n",
    "    def _fit(self, inputDF):\n",
    "        # Compute averages by hour\n",
    "        avgByHour = inputDF \\\n",
    "            .select(\"Hour\", \"NumBikesDiff+1\") \\\n",
    "            .groupBy(\"Hour\") \\\n",
    "            .agg(F.avg(\"NumBikesDiff+1\").alias(\"AvgByHour\"))\n",
    "    \n",
    "        # Compute averages by weekday\n",
    "        avgByWeekday = inputDF \\\n",
    "            .select(\"DayOfWeek\", \"NumBikesDiff+1\") \\\n",
    "            .groupBy(\"DayOfWeek\") \\\n",
    "            .agg(F.avg(\"NumBikesDiff+1\").alias(\"AvgByWeekday\"))\n",
    "        \n",
    "        # Compute averages by zone\n",
    "        avgByZone = inputDF \\\n",
    "            .select(\"Zone\", \"NumBikesDiff+1\") \\\n",
    "            .groupBy(\"Zone\") \\\n",
    "            .agg(F.avg(\"NumBikesDiff+1\").alias(\"AvgByZone\"))\n",
    "        \n",
    "        # Compute averages with all variables\n",
    "        avgCombined = inputDF \\\n",
    "            .select(\"Hour\", \"DayOfWeek\", \"Zone\", \"NumBikesDiff+1\") \\\n",
    "            .groupBy(\"Hour\", \"DayOfWeek\", \"Zone\") \\\n",
    "            .agg(F.avg(\"NumBikesDiff+1\").alias(\"AvgCombined\"))\n",
    "        \n",
    "        return TrainingSetSummarizerModel(avgByHour, avgByWeekday, avgByZone, avgCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-popularity",
   "metadata": {},
   "source": [
    "## Outlier handling\n",
    "\n",
    "Outlier detection is an essential step to improve the results of a machine learning model. In our case, a simple technique for removing outliers have been chosen: in the data exploration notebook, we have observed that less than 1% of the readings present a first-order difference lower than -4, or greater than +3 (we will consider +4 for the sake of symmetry). Thus, all the readings with a \"diff\" value lower than -4 or higher than +4 may be considered outliers.\n",
    "\n",
    "The strategy that has been identified to deal with these outliers is to saturate these observations to the two threshold values. In other words, all readings lower than -4 have been represented as -4, whereas all readings higher than +4 have been represented as +4. By removing such outliers, we aim to tolerate errors due to large oscillations of our target values, while improving the model for the average, low-difference case.\n",
    "\n",
    "*Note*: the labels with outliers removed (i.e., saturated to +4/-4) will be used during training only. o measure the actual performance of the obtained models, we will always refer to the data's label using the unmodified value; that is, the `NumBikes+1` column, which is not affected by this outlier removal phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painted-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover(Transformer):\n",
    "    def _transform(self, inputDF):\n",
    "        for col in [\"NumBikesDiff-3\", \"NumBikesDiff-2\", \"NumBikesDiff-1\", \"NumBikesDiff\", \"NumBikesDiff+1\"]:\n",
    "            inputDF = inputDF.withColumn(col, F.when(F.col(col) < -4, -4).when(F.col(col) > 4, 4).otherwise(F.col(col)))\n",
    "        return inputDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-value",
   "metadata": {},
   "source": [
    "## Data normalization\n",
    "\n",
    "Finally, we can proceed to normalize the data by removing all the redundant features, as well as those that are not relevant to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incomplete-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnwantedFeaturesRemover(Transformer):\n",
    "    def _transform(self, inputDF):\n",
    "        return inputDF.drop(\"Timestamp\", \"NumBikes-4\", \"NumBikes-3\", \"NumBikes-2\", \"NumBikes-1\", \"id\", \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-theology",
   "metadata": {},
   "source": [
    "## Pipeline definition and dataset transformation\n",
    "\n",
    "Once that all the data processing steps have been defined, we can proceed to define the actual pipeline and fitting it to the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attractive-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    DiffFeaturesExtractor(),\n",
    "    TimeFeaturesExtractor(),\n",
    "    LocationFeaturesExtractor().fit(stationsData),\n",
    "    OutlierRemover(),\n",
    "    UnwantedFeaturesRemover(),\n",
    "    TrainingSetSummarizer()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chubby-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(joinedTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interpreted-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Zone: integer (nullable = false)\n",
      " |-- StationId: integer (nullable = true)\n",
      " |-- NumBikes: integer (nullable = true)\n",
      " |-- NumBikes+1: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- NumBikesDiff-3: integer (nullable = true)\n",
      " |-- NumBikesDiff-2: integer (nullable = true)\n",
      " |-- NumBikesDiff-1: integer (nullable = true)\n",
      " |-- NumBikesDiff: integer (nullable = true)\n",
      " |-- NumBikesDiff+1: integer (nullable = true)\n",
      " |-- WorkDay: integer (nullable = false)\n",
      " |-- AvgByHour: double (nullable = true)\n",
      " |-- AvgByWeekday: double (nullable = true)\n",
      " |-- AvgByZone: double (nullable = true)\n",
      " |-- AvgCombined: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformedTrainData = model.transform(joinedTrainData)\n",
    "transformedTrainData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "looking-cleaners",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Zone: integer (nullable = false)\n",
      " |-- StationId: integer (nullable = true)\n",
      " |-- NumBikes: integer (nullable = true)\n",
      " |-- NumBikes+1: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- NumBikesDiff-3: integer (nullable = true)\n",
      " |-- NumBikesDiff-2: integer (nullable = true)\n",
      " |-- NumBikesDiff-1: integer (nullable = true)\n",
      " |-- NumBikesDiff: integer (nullable = true)\n",
      " |-- NumBikesDiff+1: integer (nullable = true)\n",
      " |-- WorkDay: integer (nullable = false)\n",
      " |-- AvgByHour: double (nullable = true)\n",
      " |-- AvgByWeekday: double (nullable = true)\n",
      " |-- AvgByZone: double (nullable = true)\n",
      " |-- AvgCombined: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformedTestData = model.transform(joinedTestData)\n",
    "transformedTrainData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-dollar",
   "metadata": {},
   "source": [
    "## Output writing\n",
    "\n",
    "Finally, we write the output datasets into an HDFS folder, from where they will be retrieved during the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suspected-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = \"/user/s291516/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "funded-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTrainData \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .save(outputPath + \"trainData/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adjustable-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTestData \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .save(outputPath + \"testData/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
